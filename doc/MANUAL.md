# Config options for the bayes_beat package

In the following, all possible parameters settings are listed and explained together with their default settings.  
 

### General

* `system` = 'BeatTracker'  
This setting is intended to be used if the simulation class was used with systems apart from the BeatTracker.  


### Path settings
  
* `base_path`  
Specifies the root folder of the bayes_beat package.
* `data_path`  
Folder where rhythm-cluster-assignments, features, etc. are stored.  
* `results_path`  
Folder where results are stored. 


### Simulation settings

* `validation_type` = 'holdout'    
    * 'holdout': use train and test set as described below  
    * 'cross_validation': use k-fold cross-validation; *train_set* and *trainLab* should describe the full dataset; the folds are generated by looking for a file with name *[train_set, '-fold', k, '.lab']* in the folder of the train labfile.  
    * 'leave_one_out': use leave-one-out splitting  
    
* `n_folds`  
Number of folds in the case that *validation_type='cross_validation'*  
* `save_inference_data` = 0  
If *true*, then save complete posterior probability to a file. This is useful for visualisations.  
* `reorganize_bars_into_cluster` = 0  
If *true*, then reorganise features into patterns as given by the cluster_assignment_file. Otherwise, the organised features are loaded from file.  
* `store_training_data` = 0  
If *true*, the features of the training data are saved to disk in order to save time when experimenting with different model parameters.  
* `stored_train_data_fln`  
This is the filename where the training data instance is saved to (and loaded from).  
* `load_training_data` = 0  
If *true*, the features of the training data are loaded from disk in order to save time when experimenting with different model parameters.  




### System parameters

* `inferenceMethod` = 'HMM_viterbi'  
Inference and model settings *{'HMM_viterbi', 'HMM_forward', 'PF'}*
* `viterbi_learning_iterations` = 0  
Number of iterations of Viterbi training (currently only for HMMs). This was used in [4].
* `tempo_tying` = 1  
Settings for Viterbi learning:  
    * = 0: p_n tied across position states (different p_n for each n)  
    * = 1: Global p_n for all changes (only one p_n)  
    * = 2: Separate p_n for tempo increase and decrease (two different p_n)  
* `model_fln`  
Filename of pre-stored model to load  
* `save_features_to_file` = 1  
If *true*, save extracted feature to a folder called *'beat_activations'* relative to the folder of the test file   
* `save_beats` = 1  
Save beat times and corresponding position within a bar (.beats.txt)  
* `save_downbeats` = 0  
Save only downbeats (.downbeats.txt)  
* `save_tempo` = 0  
Save median tempo (.bpm.txt) 
* `save_rhythm` = 0  
Save rhythm (.rhythm.txt) 
* `save_meter` = 0  
Save time_signature (.meter.txt)  
* `cluster_type` = 'meter'  
describes the way the data is clustered into various rhythmic patterns. Currently implemented: 'meter', 'rhythm', 'kmeans'
* `n_clusters`  
number of clusters for the kmeans clustering algorithm.

### Model parameters

* `n_depends_on_r` = 1  
If n_depends_on_r=true, then use different tempo limits for each rhythm state.
* `frame_length` = 0.02  
Audio frame length [sec] of the system  
* `M = 1600  
Maximum position state (used for the meter with the longest duration)  
* `N = nan`  
Number of tempo states. This setting is only relevant for the '2015' transition model. If set to *nan*
, the maximum number of tempo states is used. The maximum number depends on the tempo limits and the frame length.  
* `R = 2`  
Number of rhythmic pattern states to be used  
* `whole_note_div` = 64  
Number of position grid points per whole note. This value determines how many probability distributions (e.g., GMMs) are fitted per whole note in a bar. This is important for the observation model, as parameters are tied within this grid. For example, *whole_note_div = 64* means, that a 4/4 bar will be divided into 64 cells, and for each of the cells there will be one probability distribution (e.g., GMM) describing the features within this cell.
* `pattern_size` = 'bar'  
Cycle length of a rhythmic pattern {'beat', 'bar'} 
* `use_silence_state` = 0
Use one state that represents silence. From there, you can go to the first position of all rhythmic patterns. Currently, you can only enter a silence state at the end of a rhythmic pattern.  
* `p2s` = 0.00001  
Probability of entering the silence state  
* `pfs` = 0.001  
Probability of leaving the silence state  
* `pr` = 0
Probability of rhythmic pattern change  
* `correct_beats` = 0  
Correct beat position afterwards by shifting it to a loacl max of the onset detection function. This hwas used in the RNNBeatTracker [1] to correct for the rough discretisation of the observation model.
* `min_tempo_bpm` = 60  
Lower tempo limit in BPM (same for all rhythmic patterns).
* `max_tempo_bpm` = 230  
Upper tempo limit in BPM (same for all rhythmic patterns).
* `learn_tempo_ranges` = 1
Learn tempo ranges from training data. If *min_tempo* and *max_tempo* are set, all ranges are restricted to be between *min_tempo* and *max_tempo*  
* `tempo_outlier_percentile` = 5
When learning tempo ranges, outlier beat intervals can be ignored, e.g., *tempo_outlier_percentile = 50* uses the median tempo of all beat intervals of one song to determine the tempo ranges.  


#### Inference

* `use_mex_viterbi` = 1  
If *true*, use mex implementation of Viterbi decoding. Please make sure to compile *@HMM/viterbi.cpp* first (see README).
* `online.max_shift` = 1
In online mode (forward path), the state which is selected is chosen among a set of possible successor states. This set contains position states within a window of +/- max_shift frames
* `online.update_interval` = 200
In online mode, we reset the trayectory of selected hidden state each *update_interval* audio frames.

#### HMM parameters

* `transition_model_type` = '2015'  
Type of transition model. Can be one of the following:  
    * 'whiteley': Old transition model as proposed in [2]  
    * '2015': More efficient transition model as proposed in [3]
* `pn` = 0.001  
Probability of tempo acceleration (and deceleration) in the *whiteley* transition model.  
* `alpha` = 100
Probability of tempo acceleration (and deceleration) in the *2015* transition model. *alpha* is a squeezing factor for the (exponential) tempo change distribution  (higher values prefer a constant tempo over a tempo change from one beat to the next one). See [3] for details.


#### PF parameters
* `nParticles` = 1000  
Number of particles  
* `tempo_bpm_std` = 0.0001  
Standard deviation of the Gaussian tempo transition model in [BPM].   
* `ratio_Neff` = 0.1  
If the effective sample size is below *ratio_Neff * nParticles*, resampling is performed.  
* `res_int` = 30  
If set, resampling is performed with a fixed interval, given in audio frames.
* `resampling_scheme` = 0  
Type of resampling scheme to be used:  
    * = 0: Standard SISR (systematic resampling)  
    * = 1: Auxiliary particle filter (APF) [5]
    * = 2: Mixture PF using k-means clustering (MPF) [5]
    * = 3: Auxiliary mixture particle filter (AMPF) [5]  
* `warp_fun` = '@(x)x.^(1/4)'  
Warping function for the weights of the *APF* and *AMPF*.  
* Mixture PF parameters for *MPF* and *AMPF*. See [5] for details.  
    * `state_distance_coefficients` = [30, 1, 100]  
    Factors to adjust distance function for k-means [position, tempo, rhythmic pattern]  
    * `cluster_merging_thr` = 20  
    If distance between two cluster lower than *cluster_merging_thr*, merge these clusters.  
    * `cluster_splitting_thr` = 30  
    If the spread within a cluster is higher than *cluster_splitting_thr*, split this cluster.  
    * `n_max_clusters` = 100  
    If the number of overall clusters exceeds *n_max_clusters*, the clusters are reduced to *n_max_clusters* by removing those clusters with the lowest weight.  
    * `n_initial_clusters` = 32  
    Number of clusters to start with.  

#### Observation model
* `observationModelType` = 'MOG'  
Distribution type of the observation model:  
    * 'invGauss'  
    * 'fixed'  
    * 'gamma'  
    * 'histogram'  
    * 'multivariateHistogram'  
    * 'bivariateGauss'  
    * 'MOG' (mixture of Gaussians)  
* `feat_type` = {'lo230_superflux.mvavg', 'hi250_superflux.mvavg'}  
Cell array of feature file extensions to be used. Each dimension describes one feature.
* `online.obs_lik_floor` = 1e-7
To avoid overfitting and prevent the observation likelihood to become zero in the case of unseen data, we set a floor to the observatin likelihood. Currently, this is only implemented for the forward path.

### DATA

#### Train data
* `silence_lab`
File that holds a list of audio files with silence. This is used to learn the silence observation model params. Only needed if a *silence state* is used with the model.
* `train_set`  
Name of the training dataset.  
* `trainLab`
Path to a labfile of the training dataset. This labfile should contain all the files of the dataset.
* `clusterIdFln`  
Path to file where bar to rhythm assignments for a dataset are stored.  

#### Test data
* `test_set`  
Name of the test dataset.  
* `testLab`
Path to either a labfile of the test dataset, which should contain all the files of the dataset, or to a test song (.wav)  

# References
[1] Böck Sebastian, Florian Krebs, and Gerhard Widmer. "A multi-model approach to beat tracking considering heterogeneous music styles." Proceedings of the 15th International Society for Music Information Retrieval Conference (ISMIR), Taipei, Taiwan. 2014.

[2] Whiteley Nick, Ali Taylan Cemgil, and Simon J. Godsill. "Bayesian Modelling of Temporal Structure in Musical Audio." Proceedings of the 7th International Society for Music Information Retrieval Conference (ISMIR), Victoria, USA. 2006.

[3] Krebs Florian, Sebastian Böck, and Gerhard Widmer. "An efficient state space model for joint tempo and meter tracking." Proceedings of the 16th International Society for Music Information Retrieval Conference (ISMIR), Malaga, Spain. 2015.

[4] Krebs Florian, Filip Korzeniowski, Marten Grachten, and Gerhard Widmer. "Unsupervised learning and refinement of rhythmic patterns for beat and downbeat tracking." Proceedings of the 22nd European Signal Processing Conference (EUSIPCO), IEEE, 2014.

[5] Krebs Florian, Andre Holzapfel, Ali Taylan Cemgil, and Gerhard Widmer. "Inferring metrical structure in music using particle filters." IEEE/ACM Transactions on Audio, Speech, and Language Processing, 23.5 (2015): 817-827.